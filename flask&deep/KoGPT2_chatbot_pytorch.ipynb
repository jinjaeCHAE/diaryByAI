{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "sOqL-ERxwMDz",
    "outputId": "736237e4-5772-4a0a-d628-eca21a883317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# GPU 정보 \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "rUbPw_7Bf9Jr",
    "outputId": "7dcd19a4-b6ed-4bb1-f844-7d2e419abf50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.5.1+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-win_amd64.whl (858.4 MB)\n",
      "Collecting torchvision==0.6.1+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: future in c:\\users\\playdata\\anaconda3\\lib\\site-packages (from torch==1.5.1+cu101) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\anaconda3\\lib\\site-packages (from torch==1.5.1+cu101) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\playdata\\anaconda3\\lib\\site-packages (from torchvision==0.6.1+cu101) (8.0.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
     ]
    }
   ],
   "source": [
    "# 현재 CUDA Version에 맞는 Pytorch 설치\n",
    "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4WKGlGahhDAL",
    "outputId": "306c7476-a021-48f0-ce0b-c3a85bffd2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
      "\u001b[K     |████████████████████████████████| 68.7MB 43kB/s \n",
      "\u001b[?25hCollecting gluonnlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 37.9MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 38.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 39.2MB/s \n",
      "\u001b[?25hCollecting pytorch_lightning\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/8e/d1bb6f3696aaed40bf8263c0d9be95593dbc6a63921cca68f0e7f60e7893/pytorch_lightning-0.8.1-py3-none-any.whl (293kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 33.7MB/s \n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.20)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Collecting tokenizers==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 34.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 33.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 33.5MB/s \n",
      "\u001b[?25hCollecting PyYAML>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 37.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.5.1+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (47.3.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.6.0.post3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.29.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.17.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (1.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.4.8)\n",
      "Building wheels for collected packages: gluonnlp, sacremoses, future, PyYAML\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470038 sha256=365d143c568541d9aa8745bf1183f2377edfd391f0d801b779d0c59d81203d50\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5bdd4e1c3d42ea5c30d114f624d7c0ba5c06fc3a032b73c9db92a69b13547387\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=faa0531e81dd05ef7953578aba9a346e1d847032ed8da84a3f4acca263bc7e96\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=a21ab9d678fe198044eef0752eb978e2c464f890a3b56a0ab875821d713ccc67\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built gluonnlp sacremoses future PyYAML\n",
      "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, tokenizers, sacremoses, transformers, future, PyYAML, pytorch-lightning\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed PyYAML-5.3.1 future-0.18.2 gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 pytorch-lightning-0.8.1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치\n",
    "!pip install mxnet gluonnlp sentencepiece pandas transformers pytorch_lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "pc9gWX3SkP6G",
    "outputId": "b79d143d-908a-4914-b47d-bd0f4a3cfa86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kogpt2\n",
      "  Cloning https://github.com/SKT-AI/KoGPT2 to /tmp/pip-install-lmf90hij/kogpt2\n",
      "  Running command git clone -q https://github.com/SKT-AI/KoGPT2 /tmp/pip-install-lmf90hij/kogpt2\n",
      "Building wheels for collected packages: kogpt2\n",
      "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14052 sha256=241778ebcd2f58850e8278397b4d0132116b43e1deb6eb5691021367647a3e25\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-os9az673/wheels/2a/9f/62/3cba71a35387ff5da1d12e6b053b4d839dab0ed4310dde840d\n",
      "Successfully built kogpt2\n",
      "Installing collected packages: kogpt2\n",
      "Successfully installed kogpt2-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKT-AI/KoGPT2#egg=kogpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "P8W3gZk2ijYN",
    "outputId": "bc4ee445-1bf3-4af2-c433-375634918c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoGPT2-chatbot'...\n",
      "remote: Enumerating objects: 62, done.\u001b[K\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 62 (delta 31), reused 40 (delta 15), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (62/62), done.\n",
      "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
      "Cloning into '/content/KoGPT2-chatbot/Chatbot_data'...\n",
      "remote: Enumerating objects: 20, done.        \n",
      "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20        \n",
      "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
     ]
    }
   ],
   "source": [
    "# KoGPT2-chatbot 소스 코드 복사\n",
    "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e9ZweKmXiuaK",
    "outputId": "6cc6ce8b-105b-4f39-b865-22508865cf6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/KoGPT2-chatbot\n"
     ]
    }
   ],
   "source": [
    "# 폴더 이동\n",
    "%cd KoGPT2-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "id": "xKMZv-ZsiqkB",
    "outputId": "7cf55491-8fbd-4201-a849-9616b638d3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25 10:56:43.124599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n",
      "using cached model\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | kogpt2        | GPT2LMHeadModel  | 124 M \n",
      "1 | loss_function | CrossEntropyLoss | 0     \n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 2 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 1:   0% 0/185 [00:00<?, ?it/s] INFO:root:contexts : 다가온 이별\n",
      "INFO:root:toked ctx: ['<usr>', '▁다가온', '▁이별', '</s>', '<unused1>', '▁1', '</s>']\n",
      "INFO:root:response : 이별 예약제.\n",
      "INFO:root:contexts : 행복하기 위해 중요한 게 뭐야?\n",
      "INFO:root:toked response : ['<sys>', '▁이별', '▁예약', '제', '.', '</s>']\n",
      "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁이별', '▁예약', '제', '.', '</s>']\n",
      "INFO:root:toked ctx: ['<usr>', '▁행복', '하기', '▁위해', '▁중요한', '▁게', '▁뭐', '야', '?', '</s>', '<unused1>', '▁0', '</s>']\n",
      "INFO:root:response : 인생을 즐기는 거죠.\n",
      "INFO:root:toked response : ['<sys>', '▁인생을', '▁즐기는', '▁거죠', '.', '</s>']\n",
      "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁인생을', '▁즐기는', '▁거죠', '.', '</s>']\n",
      "Epoch 1: 100% 185/185 [03:54<00:00,  1.27s/it, loss=2.400, v_num=0]\n",
      "Epoch 00001: loss reached 2.51730 (best 2.51730), saving model to /content/KoGPT2-chatbot/model_chp/model_epoch=01-loss=2.52.ckpt as top 1\n",
      "Epoch 2:   0% 0/185 [00:00<?, ?it/s, loss=2.400, v_num=0]INFO:root:contexts : 사랑하는 사람은 딱 알아볼 수 있어?\n",
      "INFO:root:toked ctx: ['<usr>', '▁사랑하는', '▁사람은', '▁딱', '▁알아볼', '▁수', '▁있어', '?', '</s>', '<unused1>', '▁2', '</s>']\n",
      "INFO:root:response : 감정은 감출수 있는게 아니에요.\n",
      "INFO:root:toked response : ['<sys>', '▁감', '정은', '▁감', '출', '수', '▁있는', '게', '▁아니', '에요', '.', '</s>']\n",
      "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁감', '정은', '▁감', '출', '수', '▁있는', '게', '▁아니', '에요', '.', '</s>']\n",
      "INFO:root:contexts : 이미 잘못이 너무 많아서.\n",
      "INFO:root:toked ctx: ['<usr>', '▁이미', '▁잘못', '이', '▁너무', '▁많아서', '.', '</s>', '<unused1>', '▁1', '</s>']\n",
      "INFO:root:response : 더 이상의 잘못은 이해하지 못할 거예요.\n",
      "INFO:root:toked response : ['<sys>', '▁더', '▁이상의', '▁잘못', '은', '▁이해하지', '▁못할', '▁거예요', '.', '</s>']\n",
      "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁더', '▁이상의', '▁잘못', '은', '▁이해하지', '▁못할', '▁거예요', '.', '</s>']\n",
      "Epoch 2: 100% 185/185 [03:58<00:00,  1.29s/it, loss=2.078, v_num=0]\n",
      "Epoch 00002: loss reached 2.17900 (best 2.17900), saving model to /content/KoGPT2-chatbot/model_chp/model_epoch=02-loss=2.18.ckpt as top 1\n",
      "Epoch 2: 100% 185/185 [04:42<00:00,  1.52s/it, loss=2.078, v_num=0]\n",
      "INFO:root:best model path /content/KoGPT2-chatbot/model_chp/model_epoch=02-loss=2.18.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --train --gpus 1 --max_epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "f3yDcidi6wFA",
    "outputId": "b66e761c-3731-4f82-ce8b-411a65a7363d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25 11:07:27.562722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "user > 집이 너무 비싸네.\n",
      "Simsimi > 집 없는 설움을 겪으셨나봐요.\n",
      "user > 아침에 잠이 너무 많아서 걱정이야.\n",
      "Simsimi > 잠이 안 올 거예요.\n",
      "user > 비도 많이 오고 너무 더워.\n",
      "Simsimi > 시원한 음료 드세요.\n",
      "user > 비오는데 우산을 안가져 왔네..\n",
      "Simsimi > 우산 챙기세요.\n",
      "user > 비가 너무 많이와\n",
      "Simsimi > 우산 챙기세요.\n",
      "user > quit\n"
     ]
    }
   ],
   "source": [
    "# 대화 테스트, `quit`를 입력하면 대화를 종료합니다.\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoeGog7OoXFk"
   },
   "source": [
    "이 노트북은   https://colab.research.google.com/drive/1Np7d8zrch589LwwW9oX_MyzJ9jfPEvUG?usp=sharing  를 수정하여 만들었습니다..  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KoGPT2_chatbot_pytorch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
